"use strict";(self.webpackChunkmy_textbook_website=self.webpackChunkmy_textbook_website||[]).push([[695],{6535:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>c,contentTitle:()=>r,default:()=>u,frontMatter:()=>a,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"chapter-09-perception","title":"Perception Systems and Environmental Understanding","description":"Understanding perception systems for humanoid robots, including vision, audition, tactile sensing, and SLAM for environmental understanding.","source":"@site/docs/chapter-09-perception.md","sourceDirName":".","slug":"/chapter-09-perception","permalink":"/physical-ai-humanoid-robotics-textbook/docs/chapter-09-perception","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":9,"frontMatter":{"title":"Perception Systems and Environmental Understanding","description":"Understanding perception systems for humanoid robots, including vision, audition, tactile sensing, and SLAM for environmental understanding.","sidebar_position":9,"wordCount":"1400-1700","prerequisites":"Computer vision and signal processing fundamentals","learningOutcomes":["Integrate multiple sensory modalities for environmental understanding","Implement SLAM algorithms for humanoid navigation","Design perception systems that support physical interaction"],"subtopics":["Vision systems for humanoid robots","Auditory perception and sound processing","Tactile and proprioceptive sensing","Simultaneous localization and mapping (SLAM)","Scene understanding and object recognition"],"status":"draft","authors":["Textbook Author"],"reviewers":["Domain Expert"]},"sidebar":"textbookSidebar","previous":{"title":"Manipulation and Dexterous Control","permalink":"/physical-ai-humanoid-robotics-textbook/docs/chapter-08-manipulation"},"next":{"title":"Learning and Adaptation in Physical Systems","permalink":"/physical-ai-humanoid-robotics-textbook/docs/chapter-10-learning"}}');var o=i(4848),s=i(8453);const a={title:"Perception Systems and Environmental Understanding",description:"Understanding perception systems for humanoid robots, including vision, audition, tactile sensing, and SLAM for environmental understanding.",sidebar_position:9,wordCount:"1400-1700",prerequisites:"Computer vision and signal processing fundamentals",learningOutcomes:["Integrate multiple sensory modalities for environmental understanding","Implement SLAM algorithms for humanoid navigation","Design perception systems that support physical interaction"],subtopics:["Vision systems for humanoid robots","Auditory perception and sound processing","Tactile and proprioceptive sensing","Simultaneous localization and mapping (SLAM)","Scene understanding and object recognition"],status:"draft",authors:["Textbook Author"],reviewers:["Domain Expert"]},r="Perception Systems and Environmental Understanding",c={},l=[{value:"Vision Systems for Humanoid Robots",id:"vision-systems-for-humanoid-robots",level:2},{value:"Auditory Perception and Sound Processing",id:"auditory-perception-and-sound-processing",level:2},{value:"Tactile and Proprioceptive Sensing",id:"tactile-and-proprioceptive-sensing",level:2},{value:"Simultaneous Localization and Mapping (SLAM)",id:"simultaneous-localization-and-mapping-slam",level:2},{value:"Scene Understanding and Object Recognition",id:"scene-understanding-and-object-recognition",level:2},{value:"Advanced Perception Techniques",id:"advanced-perception-techniques",level:2},{value:"Summary",id:"summary",level:2}];function d(n){const e={admonition:"admonition",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,s.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"perception-systems-and-environmental-understanding",children:"Perception Systems and Environmental Understanding"})}),"\n",(0,o.jsx)(e.p,{children:"Perception systems form the foundation of environmental awareness for humanoid robots, enabling them to understand and interact with their surroundings. Unlike traditional robots that operate in structured environments, humanoid robots must navigate complex, dynamic human environments where perception accuracy and real-time performance are critical for safe and effective operation."}),"\n",(0,o.jsx)(e.p,{children:"The perception challenge for humanoid robots is multifaceted, requiring the integration of multiple sensory modalities to create a coherent understanding of the environment. This includes visual perception for object recognition and scene understanding, auditory perception for sound source localization and speech recognition, and tactile perception for physical interaction."}),"\n",(0,o.jsx)(e.h2,{id:"vision-systems-for-humanoid-robots",children:"Vision Systems for Humanoid Robots"}),"\n",(0,o.jsx)(e.p,{children:"Vision systems in humanoid robots must handle the challenges of dynamic environments, varying lighting conditions, and the need for real-time processing. Unlike static cameras in traditional robotic systems, humanoid robots have moving cameras that must coordinate with body movements while maintaining stable perception."}),"\n",(0,o.jsx)(e.p,{children:"The visual system of a humanoid robot typically includes multiple cameras (stereo vision for depth perception), dynamic attention mechanisms that focus processing resources on relevant areas, and integration with other sensory modalities to create robust environmental models."}),"\n",(0,o.jsx)(e.p,{children:"Modern humanoid robots use deep learning approaches for object recognition, scene understanding, and visual tracking. These approaches provide robust performance across varying conditions but require significant computational resources and careful training to avoid biases and failures."}),"\n",(0,o.jsx)(e.admonition,{type:"tip",children:(0,o.jsx)(e.p,{children:"Stereo vision systems in humanoid robots benefit from vergence control (the ability to adjust the angle between cameras) to focus on objects at different distances, similar to human vision."})}),"\n",(0,o.jsx)(e.h2,{id:"auditory-perception-and-sound-processing",children:"Auditory Perception and Sound Processing"}),"\n",(0,o.jsx)(e.p,{children:"Auditory perception enables humanoid robots to understand speech, localize sound sources, and detect environmental sounds that provide important contextual information. The human-like placement of microphones on a humanoid robot's head enables sound localization using interaural time and level differences."}),"\n",(0,o.jsx)(e.p,{children:"Sound source localization allows robots to identify the direction and distance of speakers or other sound sources, which is crucial for natural human-robot interaction. This involves processing audio signals from multiple microphones to determine the location of sound sources in 3D space."}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import numpy as np\nfrom scipy import signal\nimport matplotlib.pyplot as plt\nfrom scipy.spatial.distance import cdist\n\nclass AudioLocalizationSystem:\n    """\n    Audio localization system for humanoid robots\n    """\n    def __init__(self, mic_positions=None, sample_rate=44100):\n        if mic_positions is None:\n            # Default configuration: 4 microphones in a square around the head\n            self.mic_positions = np.array([\n                [0.05, 0.05, 0.1],   # Front-right\n                [-0.05, 0.05, 0.1],  # Front-left\n                [-0.05, -0.05, 0.1], # Back-left\n                [0.05, -0.05, 0.1]   # Back-right\n            ])\n        else:\n            self.mic_positions = np.array(mic_positions)\n\n        self.sample_rate = sample_rate\n        self.speed_of_sound = 343.0  # m/s\n        self.audio_buffer = {}\n\n        # Initialize for each microphone\n        for i in range(len(self.mic_positions)):\n            self.audio_buffer[i] = np.zeros(1024)  # 1024-sample buffer\n\n    def process_audio_input(self, audio_signals):\n        """\n        Process audio signals from multiple microphones\n        audio_signals: list of audio signals from each microphone\n        """\n        if len(audio_signals) != len(self.mic_positions):\n            raise ValueError("Number of audio signals must match number of microphones")\n\n        # Store the signals\n        for i, signal in enumerate(audio_signals):\n            self.audio_buffer[i] = signal\n\n        # Calculate interaural time differences (ITD)\n        itds = self._calculate_itds(audio_signals)\n\n        # Calculate interaural level differences (ILD)\n        ilds = self._calculate_ilds(audio_signals)\n\n        # Estimate sound source location\n        source_location = self._estimate_source_location(itds, ilds)\n\n        return {\n            \'source_location\': source_location,\n            \'itds\': itds,\n            \'ilds\': ilds,\n            \'confidence\': self._calculate_confidence(itds, ilds)\n        }\n\n    def _calculate_itds(self, audio_signals):\n        """\n        Calculate interaural time differences between microphone pairs\n        """\n        itds = {}\n\n        # Calculate ITD for each pair of microphones\n        for i in range(len(audio_signals)):\n            for j in range(i + 1, len(audio_signals)):\n                # Cross-correlation to find time delay\n                correlation = signal.correlate(audio_signals[i], audio_signals[j])\n                delay_samples = np.argmax(correlation) - len(audio_signals[i]) + 1\n                delay_time = delay_samples / self.sample_rate\n\n                mic_pair = (i, j)\n                itds[mic_pair] = delay_time\n\n        return itds\n\n    def _calculate_ilds(self, audio_signals):\n        """\n        Calculate interaural level differences between microphone pairs\n        """\n        ilds = {}\n\n        for i in range(len(audio_signals)):\n            for j in range(i + 1, len(audio_signals)):\n                # Calculate average power in each signal\n                power_i = np.mean(audio_signals[i]**2)\n                power_j = np.mean(audio_signals[j]**2)\n\n                # Calculate level difference in dB\n                if power_j > 0:\n                    ild = 10 * np.log10(power_i / power_j) if power_i > 0 else -100\n                else:\n                    ild = 100  # Large positive value if j is silent\n\n                mic_pair = (i, j)\n                ilds[mic_pair] = ild\n\n        return ilds\n\n    def _estimate_source_location(self, itds, ilds):\n        """\n        Estimate 3D location of sound source using ITD and ILD information\n        """\n        # Simplified approach: use ITD to estimate direction, ILD to estimate distance\n        # More sophisticated approaches would use optimization methods\n\n        if not itds:\n            return np.array([0.0, 0.0, 0.0])  # No sound detected\n\n        # Calculate approximate direction based on dominant ITD\n        # This is a simplified implementation\n        avg_azimuth = 0\n        avg_elevation = 0\n        avg_distance = 1.0  # Default distance\n\n        for (mic1, mic2), itd in itds.items():\n            # Calculate theoretical delay for a given direction\n            # Simplified spherical model\n            pos1 = self.mic_positions[mic1]\n            pos2 = self.mic_positions[mic2]\n\n            # Estimate direction based on delay and mic positions\n            baseline = pos2 - pos1\n            max_delay = np.linalg.norm(baseline) / self.speed_of_sound\n            delay_ratio = itd / max_delay if max_delay > 0 else 0\n\n            # Simplified angle calculation\n            if np.linalg.norm(baseline) > 0:\n                direction = baseline / np.linalg.norm(baseline)\n                # This is a very simplified angle calculation\n                azimuth = np.arctan2(direction[1], direction[0])\n                elevation = np.arcsin(direction[2]) if abs(direction[2]) <= 1 else 0\n\n                avg_azimuth += azimuth\n                avg_elevation += elevation\n\n        if itds:\n            avg_azimuth /= len(itds)\n            avg_elevation /= len(itds)\n\n        # Estimate distance based on ILD\n        for (mic1, mic2), ild in ilds.items():\n            # Simplified distance estimation based on level difference\n            # In reality, this would require more complex acoustic modeling\n            if abs(ild) > 1:  # Significant level difference\n                avg_distance = min(avg_distance, 2.0)  # Closer if significant difference\n\n        # Convert spherical to Cartesian coordinates\n        x = avg_distance * np.cos(avg_elevation) * np.cos(avg_azimuth)\n        y = avg_distance * np.cos(avg_elevation) * np.sin(avg_azimuth)\n        z = avg_distance * np.sin(avg_elevation)\n\n        return np.array([x, y, z])\n\n    def _calculate_confidence(self, itds, ilds):\n        """\n        Calculate confidence in sound source localization\n        """\n        # Confidence based on consistency of ITD and ILD measurements\n        if not itds:\n            return 0.0\n\n        # Calculate variance of ITD measurements\n        itd_values = list(itds.values())\n        itd_variance = np.var(itd_values) if len(itd_values) > 1 else 0\n\n        # Lower variance means higher confidence\n        confidence = max(0, 1 - itd_variance * 1000)  # Arbitrary scaling\n\n        return confidence\n\nclass VisualPerceptionSystem:\n    """\n    Visual perception system for humanoid robots\n    """\n    def __init__(self, camera_config=None):\n        if camera_config is None:\n            # Default stereo camera configuration\n            self.camera_config = {\n                \'left_camera\': {\n                    \'position\': np.array([-0.06, 0, 0]),  # 6cm baseline\n                    \'orientation\': np.eye(3),\n                    \'fov\': 60,  # degrees\n                    \'resolution\': (640, 480)\n                },\n                \'right_camera\': {\n                    \'position\': np.array([0.06, 0, 0]),   # 6cm baseline\n                    \'orientation\': np.eye(3),\n                    \'fov\': 60,  # degrees\n                    \'resolution\': (640, 480)\n                }\n            }\n        else:\n            self.camera_config = camera_config\n\n        self.intrinsic_matrix = self._calculate_intrinsic_matrix()\n        self.extrinsics = self._calculate_extrinsics()\n\n    def _calculate_intrinsic_matrix(self):\n        """\n        Calculate camera intrinsic matrix\n        """\n        # Simplified intrinsic matrix calculation\n        fov = np.radians(self.camera_config[\'left_camera\'][\'fov\'])\n        width, height = self.camera_config[\'left_camera\'][\'resolution\']\n\n        # Calculate focal length from FOV\n        focal_length = (width / 2) / np.tan(fov / 2)\n\n        # Intrinsic matrix\n        K = np.array([\n            [focal_length, 0, width / 2],\n            [0, focal_length, height / 2],\n            [0, 0, 1]\n        ])\n\n        return K\n\n    def _calculate_extrinsics(self):\n        """\n        Calculate extrinsic parameters between cameras\n        """\n        # For stereo system, calculate transformation from left to right camera\n        left_pos = self.camera_config[\'left_camera\'][\'position\']\n        right_pos = self.camera_config[\'right_camera\'][\'position\']\n\n        # Translation vector\n        T = right_pos - left_pos\n\n        # Rotation (assuming cameras are aligned)\n        R = np.eye(3)\n\n        return {\'R\': R, \'T\': T}\n\n    def stereo_depth_estimation(self, left_image, right_image):\n        """\n        Estimate depth using stereo vision\n        """\n        # Simplified stereo depth estimation\n        # In practice, this would use more sophisticated algorithms like SGBM or deep learning\n\n        # Convert to grayscale if needed\n        if len(left_image.shape) == 3:\n            left_gray = np.mean(left_image, axis=2)\n            right_gray = np.mean(right_image, axis=2)\n        else:\n            left_gray = left_image\n            right_gray = right_image\n\n        # Simplified block matching for disparity calculation\n        block_size = 15\n        max_disparity = 64\n\n        # Calculate disparity map using normalized cross-correlation\n        height, width = left_gray.shape\n        disparity_map = np.zeros((height, width))\n\n        for y in range(block_size//2, height - block_size//2):\n            for x in range(block_size//2, width - max_disparity - block_size//2):\n                best_match = -1\n                best_disparity = 0\n\n                left_block = left_gray[y-block_size//2:y+block_size//2+1,\n                                     x-block_size//2:x+block_size//2+1]\n\n                for d in range(max_disparity):\n                    if x - d - block_size//2 < 0:\n                        continue\n\n                    right_block = right_gray[y-block_size//2:y+block_size//2+1,\n                                           x-d-block_size//2:x-d+block_size//2+1]\n\n                    # Calculate NCC\n                    if left_block.shape == right_block.shape:\n                        ncc = self._normalized_cross_correlation(left_block, right_block)\n                        if ncc > best_match:\n                            best_match = ncc\n                            best_disparity = d\n\n                disparity_map[y, x] = best_disparity\n\n        # Convert disparity to depth\n        baseline = np.linalg.norm(self.extrinsics[\'T\'])  # Camera baseline\n        depth_map = (self.intrinsic_matrix[0, 0] * baseline) / (disparity_map + 1e-6)\n\n        return depth_map\n\n    def _normalized_cross_correlation(self, img1, img2):\n        """\n        Calculate normalized cross-correlation between two images\n        """\n        mean1 = np.mean(img1)\n        mean2 = np.mean(img2)\n\n        img1_centered = img1 - mean1\n        img2_centered = img2 - mean2\n\n        numerator = np.sum(img1_centered * img2_centered)\n        denominator = np.sqrt(np.sum(img1_centered**2) * np.sum(img2_centered**2))\n\n        if denominator == 0:\n            return 0\n        else:\n            return numerator / denominator\n\n    def object_detection(self, image):\n        """\n        Detect objects in the image\n        """\n        # Simplified object detection using basic computer vision techniques\n        # In practice, this would use deep learning models like YOLO or R-CNN\n\n        # Convert to grayscale\n        if len(image.shape) == 3:\n            gray = np.mean(image, axis=2)\n        else:\n            gray = image\n\n        # Apply edge detection\n        edges = self._sobel_edge_detection(gray)\n\n        # Find contours\n        contours = self._find_contours(edges)\n\n        # Classify objects based on shape and size\n        objects = []\n        for contour in contours:\n            if len(contour) > 10:  # Minimum contour size\n                # Calculate bounding box\n                x, y, w, h = self._bounding_box(contour)\n\n                # Calculate aspect ratio and other features\n                aspect_ratio = w / h if h != 0 else 0\n                area = w * h\n\n                # Simple classification based on size and shape\n                if area > 1000:  # Minimum area threshold\n                    object_type = self._classify_object(w, h, aspect_ratio)\n                    objects.append({\n                        \'type\': object_type,\n                        \'bbox\': (x, y, w, h),\n                        \'center\': (x + w/2, y + h/2),\n                        \'confidence\': 0.8  # Simplified confidence\n                    })\n\n        return objects\n\n    def _sobel_edge_detection(self, image):\n        """\n        Apply Sobel edge detection\n        """\n        # Sobel kernels\n        sobel_x = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]])\n        sobel_y = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]])\n\n        # Apply convolution\n        grad_x = signal.convolve2d(image, sobel_x, mode=\'same\')\n        grad_y = signal.convolve2d(image, sobel_y, mode=\'same\')\n\n        # Calculate gradient magnitude\n        edges = np.sqrt(grad_x**2 + grad_y**2)\n\n        # Apply threshold\n        edges = (edges > np.mean(edges)) * 255\n\n        return edges\n\n    def _find_contours(self, edges):\n        """\n        Find contours in edge image (simplified)\n        """\n        # Simplified contour finding using connected components\n        visited = np.zeros_like(edges)\n        contours = []\n\n        height, width = edges.shape\n\n        for y in range(height):\n            for x in range(width):\n                if edges[y, x] > 0 and not visited[y, x]:\n                    contour = self._flood_fill(edges, visited, x, y)\n                    if len(contour) > 5:  # Minimum contour size\n                        contours.append(contour)\n\n        return contours\n\n    def _flood_fill(self, edges, visited, start_x, start_y):\n        """\n        Flood fill algorithm to find connected components\n        """\n        contour = []\n        stack = [(start_x, start_y)]\n\n        height, width = edges.shape\n\n        while stack:\n            x, y = stack.pop()\n\n            if (x < 0 or x >= width or y < 0 or y >= height or\n                visited[y, x] or edges[y, x] == 0):\n                continue\n\n            visited[y, x] = True\n            contour.append((x, y))\n\n            # Add neighbors\n            for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n                stack.append((x + dx, y + dy))\n\n        return contour\n\n    def _bounding_box(self, points):\n        """\n        Calculate bounding box for a set of points\n        """\n        xs = [p[0] for p in points]\n        ys = [p[1] for p in points]\n\n        x_min, x_max = min(xs), max(xs)\n        y_min, y_max = min(ys), max(ys)\n\n        return x_min, y_min, x_max - x_min, y_max - y_min\n\n    def _classify_object(self, width, height, aspect_ratio):\n        """\n        Simple object classification based on shape\n        """\n        area = width * height\n\n        if aspect_ratio > 2.0 or aspect_ratio < 0.5:\n            return "elongated_object"  # Bottle, pencil, etc.\n        elif 0.8 <= aspect_ratio <= 1.2:\n            return "round_object"      # Ball, cup, etc.\n        else:\n            return "rectangular_object" # Box, book, etc.\n'})}),"\n",(0,o.jsx)(e.h2,{id:"tactile-and-proprioceptive-sensing",children:"Tactile and Proprioceptive Sensing"}),"\n",(0,o.jsx)(e.p,{children:"Tactile sensing in humanoid robots provides crucial information about physical interactions with objects and the environment. This includes force sensing for grip control, slip detection for preventing object dropping, and texture recognition for object identification."}),"\n",(0,o.jsx)(e.p,{children:"Proprioceptive sensing provides information about the robot's own body configuration, including joint angles, velocities, and forces. This information is essential for coordinated movement and balance control."}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-cpp",children:'#include <Eigen/Dense>\n#include <vector>\n#include <map>\n#include <string>\n#include <chrono>\n\nclass TactileSensorArray {\npublic:\n    struct TactileData {\n        Eigen::Vector3d position;     // Position in sensor frame\n        Eigen::Vector3d force;        // 3D force vector\n        double pressure;              // Pressure magnitude\n        double temperature;           // Temperature reading\n        bool contact;                 // Whether contact is detected\n        double timestamp;             // Timestamp of measurement\n    };\n\nprivate:\n    std::vector<TactileData> sensors;\n    std::string location;  // Where the sensor array is located (e.g., "right_hand")\n    double contact_threshold;\n    double slip_threshold;\n\npublic:\n    TactileSensorArray(int num_sensors, const std::string& loc, double contact_thresh = 0.1)\n        : location(loc), contact_threshold(contact_thresh), slip_threshold(0.05) {\n        sensors.resize(num_sensors);\n        for (auto& sensor : sensors) {\n            sensor.contact = false;\n            sensor.pressure = 0.0;\n            sensor.temperature = 25.0;  // Room temperature\n        }\n    }\n\n    void updateSensors(const std::vector<Eigen::Vector4d>& raw_data) {\n        if (raw_data.size() != sensors.size()) {\n            throw std::runtime_error("Raw data size doesn\'t match sensor count");\n        }\n\n        auto now = std::chrono::high_resolution_clock::now();\n        double timestamp = std::chrono::duration<double>(\n            now.time_since_epoch()).count();\n\n        for (size_t i = 0; i < raw_data.size(); ++i) {\n            sensors[i].position << raw_data[i][0], raw_data[i][1], raw_data[i][2];\n            sensors[i].pressure = raw_data[i][3];\n            sensors[i].contact = (raw_data[i][3] > contact_threshold);\n            sensors[i].timestamp = timestamp;\n\n            // Calculate force vector from pressure and position (simplified)\n            sensors[i].force = sensors[i].position * sensors[i].pressure * 0.001;\n        }\n    }\n\n    std::vector<int> getContactSensors() const {\n        std::vector<int> contacts;\n        for (size_t i = 0; i < sensors.size(); ++i) {\n            if (sensors[i].contact) {\n                contacts.push_back(i);\n            }\n        }\n        return contacts;\n    }\n\n    double getTotalForce() const {\n        double total_force = 0.0;\n        for (const auto& sensor : sensors) {\n            total_force += sensor.force.norm();\n        }\n        return total_force;\n    }\n\n    bool detectSlip() const {\n        // Simplified slip detection based on variance in pressure readings\n        if (sensors.empty()) return false;\n\n        std::vector<double> pressures;\n        for (const auto& sensor : sensors) {\n            if (sensor.contact) {\n                pressures.push_back(sensor.pressure);\n            }\n        }\n\n        if (pressures.size() < 2) return false;\n\n        // Calculate variance\n        double mean = 0.0;\n        for (double p : pressures) {\n            mean += p;\n        }\n        mean /= pressures.size();\n\n        double variance = 0.0;\n        for (double p : pressures) {\n            variance += (p - mean) * (p - mean);\n        }\n        variance /= pressures.size();\n\n        return variance > slip_threshold;\n    }\n\n    Eigen::Vector3d getCenterOfPressure() const {\n        Eigen::Vector3d total_force = Eigen::Vector3d::Zero();\n        Eigen::Vector3d weighted_position = Eigen::Vector3d::Zero();\n\n        for (const auto& sensor : sensors) {\n            if (sensor.contact) {\n                total_force += sensor.force;\n                weighted_position += sensor.position * sensor.force.norm();\n            }\n        }\n\n        if (total_force.norm() > 1e-6) {\n            return weighted_position / total_force.norm();\n        } else {\n            return Eigen::Vector3d::Zero();\n        }\n    }\n};\n\nclass ProprioceptiveSystem {\nprivate:\n    std::vector<double> joint_positions;\n    std::vector<double> joint_velocities;\n    std::vector<double> joint_torques;\n    std::vector<std::string> joint_names;\n    double timestamp;\n\npublic:\n    ProprioceptiveSystem(const std::vector<std::string>& names)\n        : joint_names(names) {\n        joint_positions.resize(names.size(), 0.0);\n        joint_velocities.resize(names.size(), 0.0);\n        joint_torques.resize(names.size(), 0.0);\n\n        auto now = std::chrono::high_resolution_clock::now();\n        timestamp = std::chrono::duration<double>(now.time_since_epoch()).count();\n    }\n\n    void updateSensors(const std::vector<double>& positions,\n                      const std::vector<double>& velocities,\n                      const std::vector<double>& torques) {\n        if (positions.size() != joint_positions.size() ||\n            velocities.size() != joint_velocities.size() ||\n            torques.size() != joint_torques.size()) {\n            throw std::runtime_error("Sensor data size mismatch");\n        }\n\n        joint_positions = positions;\n        joint_velocities = velocities;\n        joint_torques = torques;\n\n        auto now = std::chrono::high_resolution_clock::now();\n        timestamp = std::chrono::duration<double>(now.time_since_epoch()).count();\n    }\n\n    double getJointPosition(int joint_index) const {\n        if (joint_index >= 0 && joint_index < joint_positions.size()) {\n            return joint_positions[joint_index];\n        }\n        return 0.0;\n    }\n\n    double getJointVelocity(int joint_index) const {\n        if (joint_index >= 0 && joint_index < joint_velocities.size()) {\n            return joint_velocities[joint_index];\n        }\n        return 0.0;\n    }\n\n    double getJointTorque(int joint_index) const {\n        if (joint_index >= 0 && joint_index < joint_torques.size()) {\n            return joint_torques[joint_index];\n        }\n        return 0.0;\n    }\n\n    std::vector<double> getLimbPosition(const std::vector<int>& joint_indices) const {\n        std::vector<double> positions;\n        for (int idx : joint_indices) {\n            if (idx >= 0 && idx < joint_positions.size()) {\n                positions.push_back(joint_positions[idx]);\n            }\n        }\n        return positions;\n    }\n\n    bool isJointLimitExceeded(int joint_index, double limit) const {\n        if (joint_index >= 0 && joint_index < joint_positions.size()) {\n            return std::abs(joint_positions[joint_index]) > limit;\n        }\n        return false;\n    }\n};\n'})}),"\n",(0,o.jsx)(e.h2,{id:"simultaneous-localization-and-mapping-slam",children:"Simultaneous Localization and Mapping (SLAM)"}),"\n",(0,o.jsx)(e.p,{children:"SLAM is critical for humanoid robots that must navigate unknown environments. The system must simultaneously build a map of the environment and determine the robot's location within that map. This is particularly challenging for humanoid robots due to their complex movement patterns and the need to maintain balance while mapping."}),"\n",(0,o.jsx)(e.p,{children:"Modern SLAM systems for humanoid robots often use visual-inertial approaches that combine camera data with inertial measurement units (IMUs) to achieve robust localization even during dynamic movements."}),"\n",(0,o.jsx)(e.h2,{id:"scene-understanding-and-object-recognition",children:"Scene Understanding and Object Recognition"}),"\n",(0,o.jsx)(e.p,{children:"Scene understanding goes beyond simple object detection to include understanding the spatial relationships between objects, their functional properties, and the appropriate actions that can be performed with them. This is essential for humanoid robots that must operate in human environments where context is crucial for appropriate behavior."}),"\n",(0,o.jsx)(e.p,{children:"[Image: Reference to diagram or illustration]"}),"\n",(0,o.jsx)(e.h2,{id:"advanced-perception-techniques",children:"Advanced Perception Techniques"}),"\n",(0,o.jsx)(e.p,{children:"Modern humanoid robots employ several advanced perception techniques:"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Deep Learning for Perception"}),": Using neural networks for object recognition, scene understanding, and sensor fusion"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Event-Based Vision"}),": Using dynamic vision sensors that respond to changes in the environment"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Multi-Modal Fusion"}),": Integrating information from multiple sensory modalities"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Active Perception"}),": Controlling sensors to actively gather information"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Predictive Perception"}),": Anticipating future states based on current observations"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(e.p,{children:"Perception systems in humanoid robots must integrate multiple sensory modalities to create a coherent understanding of complex, dynamic environments. The challenge lies in achieving real-time performance, robustness to varying conditions, and the ability to understand both objects and their functional relationships. Success in humanoid perception will require continued advances in sensor technology, signal processing, and machine learning that work together to create human-like environmental awareness capabilities."})]})}function u(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>a,x:()=>r});var t=i(6540);const o={},s=t.createContext(o);function a(n){const e=t.useContext(s);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:a(n.components),t.createElement(s.Provider,{value:e},n.children)}}}]);