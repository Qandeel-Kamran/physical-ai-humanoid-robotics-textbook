"use strict";(self.webpackChunkmy_textbook_website=self.webpackChunkmy_textbook_website||[]).push([[629],{6565:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>m,frontMatter:()=>r,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"chapter-10-learning","title":"Learning and Adaptation in Physical Systems","description":"Understanding learning algorithms and adaptation mechanisms for humanoid robots, including reinforcement learning, imitation learning, and transfer learning.","source":"@site/docs/chapter-10-learning.md","sourceDirName":".","slug":"/chapter-10-learning","permalink":"/physical-ai-humanoid-robotics-textbook/docs/chapter-10-learning","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":10,"frontMatter":{"title":"Learning and Adaptation in Physical Systems","description":"Understanding learning algorithms and adaptation mechanisms for humanoid robots, including reinforcement learning, imitation learning, and transfer learning.","sidebar_position":10,"wordCount":"1600-1900","prerequisites":"Machine learning and AI fundamentals","learningOutcomes":["Apply reinforcement learning to physical control tasks","Implement imitation learning from human demonstrations","Design systems that adapt to new physical environments"],"subtopics":["Reinforcement learning for physical tasks","Imitation learning from human demonstrations","Transfer learning between simulation and reality","Online learning and adaptation","Skill acquisition and refinement"],"status":"draft","authors":["Textbook Author"],"reviewers":["Domain Expert"]},"sidebar":"textbookSidebar","previous":{"title":"Perception Systems and Environmental Understanding","permalink":"/physical-ai-humanoid-robotics-textbook/docs/chapter-09-perception"},"next":{"title":"Human-Robot Interaction and Social Robotics","permalink":"/physical-ai-humanoid-robotics-textbook/docs/chapter-11-hri"}}');var i=t(4848),o=t(8453);const r={title:"Learning and Adaptation in Physical Systems",description:"Understanding learning algorithms and adaptation mechanisms for humanoid robots, including reinforcement learning, imitation learning, and transfer learning.",sidebar_position:10,wordCount:"1600-1900",prerequisites:"Machine learning and AI fundamentals",learningOutcomes:["Apply reinforcement learning to physical control tasks","Implement imitation learning from human demonstrations","Design systems that adapt to new physical environments"],subtopics:["Reinforcement learning for physical tasks","Imitation learning from human demonstrations","Transfer learning between simulation and reality","Online learning and adaptation","Skill acquisition and refinement"],status:"draft",authors:["Textbook Author"],reviewers:["Domain Expert"]},s="Learning and Adaptation in Physical Systems",l={},c=[{value:"Reinforcement Learning for Physical Tasks",id:"reinforcement-learning-for-physical-tasks",level:2},{value:"Imitation Learning from Human Demonstrations",id:"imitation-learning-from-human-demonstrations",level:2},{value:"Transfer Learning Between Simulation and Reality",id:"transfer-learning-between-simulation-and-reality",level:2},{value:"Online Learning and Adaptation",id:"online-learning-and-adaptation",level:2},{value:"Skill Acquisition and Refinement",id:"skill-acquisition-and-refinement",level:2},{value:"Advanced Learning Techniques",id:"advanced-learning-techniques",level:2},{value:"Summary",id:"summary",level:2}];function d(n){const e={admonition:"admonition",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,o.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"learning-and-adaptation-in-physical-systems",children:"Learning and Adaptation in Physical Systems"})}),"\n",(0,i.jsx)(e.p,{children:"Learning and adaptation are fundamental capabilities that enable humanoid robots to improve their performance over time and adapt to new situations. Unlike traditional robots that execute pre-programmed behaviors, humanoid robots must be capable of learning from experience, adapting to environmental changes, and refining their skills through practice."}),"\n",(0,i.jsx)(e.p,{children:"The challenge in learning for physical systems lies in the need to balance exploration with safety, handle the high-dimensional state and action spaces characteristic of humanoid robots, and ensure that learning processes converge to stable and safe behaviors. Physical systems also face the reality of embodiment constraints, sensor noise, and the need for real-time performance."}),"\n",(0,i.jsx)(e.h2,{id:"reinforcement-learning-for-physical-tasks",children:"Reinforcement Learning for Physical Tasks"}),"\n",(0,i.jsx)(e.p,{children:"Reinforcement learning (RL) provides a framework for learning control policies through interaction with the environment. In the context of humanoid robots, RL can be used to learn complex behaviors such as walking, manipulation, and interaction with objects."}),"\n",(0,i.jsx)(e.p,{children:"Deep reinforcement learning has shown particular promise for humanoid robotics, with deep neural networks serving as function approximators for complex state and action spaces. However, applying RL to physical systems requires careful consideration of safety constraints, sample efficiency, and the reality gap between simulation and the real world."}),"\n",(0,i.jsx)(e.admonition,{type:"tip",children:(0,i.jsx)(e.p,{children:"When applying reinforcement learning to physical systems, it's crucial to implement safety constraints and exploration bounds to prevent dangerous behaviors during learning. Model-based approaches can improve sample efficiency by learning environment models."})}),"\n",(0,i.jsx)(e.h2,{id:"imitation-learning-from-human-demonstrations",children:"Imitation Learning from Human Demonstrations"}),"\n",(0,i.jsx)(e.p,{children:"Imitation learning enables humanoid robots to acquire skills by observing and replicating human demonstrations. This approach can significantly reduce the amount of training time required compared to learning from scratch, as the robot can bootstrap its learning from human expertise."}),"\n",(0,i.jsx)(e.p,{children:"Imitation learning typically involves learning a mapping from observed states to actions. This can be approached through behavioral cloning, where the robot learns to directly imitate demonstrated actions, or through inverse reinforcement learning, where the robot attempts to infer the reward function that motivated the demonstrated behavior."}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom collections import deque\nimport random\n\nclass HumanoidEnvironment:\n    """\n    Simplified environment for humanoid robot learning\n    """\n    def __init__(self):\n        # State: [x_pos, y_pos, z_pos, roll, pitch, yaw, x_vel, y_vel, z_vel, roll_rate, pitch_rate, yaw_rate]\n        self.state_dim = 12\n        # Action: joint torques for simplified humanoid\n        self.action_dim = 10\n        self.state = np.zeros(self.state_dim)\n        self.max_steps = 1000\n        self.current_step = 0\n\n    def reset(self):\n        self.state = np.random.randn(self.state_dim) * 0.1\n        self.current_step = 0\n        return self.state\n\n    def step(self, action):\n        # Simplified dynamics update\n        # In reality, this would involve complex physics simulation\n        dt = 0.01  # Time step\n\n        # Update state based on action (simplified)\n        new_state = self.state.copy()\n\n        # Apply action as change to certain state variables\n        new_state[6:9] += action[:3] * dt  # Change in linear velocity\n        new_state[9:12] += action[3:6] * dt  # Change in angular velocity\n        new_state[0:3] += new_state[6:9] * dt  # Update position\n        new_state[3:6] += new_state[9:12] * dt  # Update orientation (simplified)\n\n        # Add some damping\n        new_state[6:9] *= 0.99\n        new_state[9:12] *= 0.99\n\n        self.state = new_state\n        self.current_step += 1\n\n        # Calculate reward (example: staying upright and moving forward)\n        reward = self.calculate_reward()\n\n        done = self.current_step >= self.max_steps\n        info = {}\n\n        return self.state, reward, done, info\n\n    def calculate_reward(self):\n        """\n        Calculate reward based on current state\n        """\n        # Reward for staying upright (z > 0.5)\n        upright_reward = max(0, self.state[2] - 0.5) * 10\n\n        # Penalty for falling over (roll or pitch too large)\n        tilt_penalty = min(0, (0.3 - abs(self.state[3]))) * 5  # Roll\n        tilt_penalty += min(0, (0.3 - abs(self.state[4]))) * 5  # Pitch\n\n        # Reward for forward movement\n        forward_reward = max(0, self.state[6]) * 5  # X velocity\n\n        # Penalty for excessive joint velocities\n        velocity_penalty = -np.sum(np.abs(self.state[6:12])) * 0.1\n\n        total_reward = upright_reward + tilt_penalty + forward_reward + velocity_penalty\n        return total_reward\n\nclass PolicyNetwork(nn.Module):\n    """\n    Neural network for policy in reinforcement learning\n    """\n    def __init__(self, state_dim, action_dim, hidden_dim=256):\n        super(PolicyNetwork, self).__init__()\n        self.fc1 = nn.Linear(state_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n        self.fc_mean = nn.Linear(hidden_dim, action_dim)\n        self.fc_std = nn.Linear(hidden_dim, action_dim)\n\n        # Initialize weights\n        nn.init.xavier_uniform_(self.fc1.weight)\n        nn.init.xavier_uniform_(self.fc2.weight)\n        nn.init.xavier_uniform_(self.fc3.weight)\n        nn.init.xavier_uniform_(self.fc_mean.weight)\n        nn.init.xavier_uniform_(self.fc_std.weight)\n\n    def forward(self, state):\n        x = torch.relu(self.fc1(state))\n        x = torch.relu(self.fc2(x))\n        x = torch.relu(self.fc3(x))\n\n        mean = torch.tanh(self.fc_mean(x))  # Actions between -1 and 1\n        std = torch.sigmoid(self.fc_std(x))  # Standard deviation between 0 and 1\n\n        return mean, std\n\nclass ValueNetwork(nn.Module):\n    """\n    Neural network for value function in reinforcement learning\n    """\n    def __init__(self, state_dim, hidden_dim=256):\n        super(ValueNetwork, self).__init__()\n        self.fc1 = nn.Linear(state_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n        self.fc_out = nn.Linear(hidden_dim, 1)\n\n    def forward(self, state):\n        x = torch.relu(self.fc1(state))\n        x = torch.relu(self.fc2(x))\n        x = torch.relu(self.fc3(x))\n        value = self.fc_out(x)\n        return value\n\nclass PPOAgent:\n    """\n    Proximal Policy Optimization (PPO) agent for humanoid learning\n    """\n    def __init__(self, state_dim, action_dim, lr_actor=3e-4, lr_critic=1e-3, gamma=0.99, eps_clip=0.2):\n        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n\n        self.gamma = gamma\n        self.eps_clip = eps_clip\n\n        # Actor and critic networks\n        self.actor = PolicyNetwork(state_dim, action_dim).to(self.device)\n        self.critic = ValueNetwork(state_dim).to(self.device)\n\n        # Optimizers\n        self.optimizer_actor = optim.Adam(self.actor.parameters(), lr=lr_actor)\n        self.optimizer_critic = optim.Adam(self.critic.parameters(), lr=lr_critic)\n\n        # Training parameters\n        self.buffer = []\n\n    def select_action(self, state):\n        """\n        Select action using current policy\n        """\n        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n\n        with torch.no_grad():\n            mean, std = self.actor(state_tensor)\n\n        # Sample action from Gaussian distribution\n        dist = torch.distributions.Normal(mean, std)\n        action = dist.sample()\n        log_prob = dist.log_prob(action)\n\n        return action.cpu().numpy()[0], log_prob.cpu().numpy()[0]\n\n    def evaluate(self, state, action):\n        """\n        Evaluate action probability and state value\n        """\n        mean, std = self.actor(state)\n        dist = torch.distributions.Normal(mean, std)\n\n        action_logprobs = dist.log_prob(action)\n        dist_entropy = dist.entropy()\n\n        state_values = self.critic(state)\n\n        return action_logprobs, torch.squeeze(state_values), dist_entropy\n\n    def update(self, states, actions, rewards, logprobs, state_values, terminals):\n        """\n        Update actor and critic networks\n        """\n        # Convert to tensors\n        states = torch.FloatTensor(states).to(self.device)\n        actions = torch.FloatTensor(actions).to(self.device)\n        rewards = torch.FloatTensor(rewards).to(self.device)\n        logprobs = torch.FloatTensor(logprobs).to(self.device)\n        state_values = torch.FloatTensor(state_values).to(self.device)\n        terminals = torch.BoolTensor(terminals).to(self.device)\n\n        # Calculate discounted rewards\n        discounted_rewards = []\n        running_reward = 0\n\n        for i in reversed(range(len(rewards))):\n            running_reward = rewards[i] + self.gamma * running_reward * (not terminals[i])\n            discounted_rewards.insert(0, running_reward)\n\n        discounted_rewards = torch.FloatTensor(discounted_rewards).to(self.device)\n\n        # Normalize discounted rewards\n        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-7)\n\n        # Calculate advantages\n        advantages = discounted_rewards - state_values.detach()\n        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-7)\n\n        # Optimize policy for K epochs\n        for _ in range(3):  # K epochs\n            # Evaluate actions at current policy\n            logprobs_new, state_values_new, entropy = self.evaluate(states, actions)\n\n            # Calculate ratios\n            ratios = torch.exp(logprobs_new - logprobs.detach())\n\n            # Calculate surrogates\n            surr1 = ratios * advantages\n            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n\n            # Actor loss\n            actor_loss = -torch.min(surr1, surr2).mean() - 0.01 * entropy.mean()\n\n            # Critic loss\n            critic_loss = nn.MSELoss()(state_values_new, discounted_rewards)\n\n            # Update networks\n            self.optimizer_actor.zero_grad()\n            actor_loss.backward()\n            self.optimizer_actor.step()\n\n            self.optimizer_critic.zero_grad()\n            critic_loss.backward()\n            self.optimizer_critic.step()\n\n    def save_model(self, filepath):\n        """\n        Save trained model\n        """\n        torch.save({\n            \'actor_state_dict\': self.actor.state_dict(),\n            \'critic_state_dict\': self.critic.state_dict(),\n            \'optimizer_actor_state_dict\': self.optimizer_actor.state_dict(),\n            \'optimizer_critic_state_dict\': self.optimizer_critic.state_dict()\n        }, filepath)\n\n    def load_model(self, filepath):\n        """\n        Load trained model\n        """\n        checkpoint = torch.load(filepath, map_location=self.device)\n        self.actor.load_state_dict(checkpoint[\'actor_state_dict\'])\n        self.critic.load_state_dict(checkpoint[\'critic_state_dict\'])\n        self.optimizer_actor.load_state_dict(checkpoint[\'optimizer_actor_state_dict\'])\n        self.optimizer_critic.load_state_dict(checkpoint[\'optimizer_critic_state_dict\'])\n\nclass ImitationLearningAgent:\n    """\n    Behavioral cloning agent for learning from demonstrations\n    """\n    def __init__(self, state_dim, action_dim, learning_rate=1e-3):\n        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n\n        # Network for behavioral cloning\n        self.network = nn.Sequential(\n            nn.Linear(state_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            nn.Linear(256, action_dim),\n            nn.Tanh()  # Actions between -1 and 1\n        ).to(self.device)\n\n        self.optimizer = optim.Adam(self.network.parameters(), lr=learning_rate)\n        self.criterion = nn.MSELoss()\n\n        # Storage for demonstration data\n        self.demo_states = []\n        self.demo_actions = []\n\n    def add_demonstration(self, states, actions):\n        """\n        Add demonstration data\n        """\n        self.demo_states.extend(states)\n        self.demo_actions.extend(actions)\n\n    def train(self, epochs=100, batch_size=64):\n        """\n        Train the imitation learning model\n        """\n        if len(self.demo_states) == 0:\n            print("No demonstration data available!")\n            return\n\n        states = torch.FloatTensor(self.demo_states).to(self.device)\n        actions = torch.FloatTensor(self.demo_actions).to(self.device)\n\n        dataset = torch.utils.data.TensorDataset(states, actions)\n        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n        for epoch in range(epochs):\n            total_loss = 0\n            for batch_states, batch_actions in dataloader:\n                self.optimizer.zero_grad()\n\n                predicted_actions = self.network(batch_states)\n                loss = self.criterion(predicted_actions, batch_actions)\n\n                loss.backward()\n                self.optimizer.step()\n\n                total_loss += loss.item()\n\n            if epoch % 20 == 0:\n                print(f"Epoch {epoch}, Loss: {total_loss/len(dataloader):.4f}")\n\n    def predict_action(self, state):\n        """\n        Predict action for given state\n        """\n        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n\n        with torch.no_grad():\n            action = self.network(state_tensor)\n\n        return action.cpu().numpy()[0]\n\n    def finetune_with_rl(self, env, rl_agent, steps=10000):\n        """\n        Fine-tune imitation learning with reinforcement learning\n        """\n        print("Fine-tuning with reinforcement learning...")\n\n        state = env.reset()\n        total_reward = 0\n\n        for step in range(steps):\n            # Use imitation policy to get action\n            action = self.predict_action(state)\n\n            # Add some exploration\n            action += np.random.normal(0, 0.1, size=action.shape)\n\n            next_state, reward, done, info = env.step(action)\n\n            # Add to RL agent\'s buffer\n            rl_agent.buffer.append((state, action, reward, done))\n\n            state = next_state\n            total_reward += reward\n\n            if done:\n                state = env.reset()\n\n            # Update RL agent periodically\n            if len(rl_agent.buffer) >= 1000:\n                # This would involve updating the RL agent with collected experience\n                # Implementation would depend on specific RL algorithm\n                rl_agent.buffer = []  # Reset buffer after update\n\n            if step % 1000 == 0:\n                print(f"Step {step}, Average Reward: {total_reward/max(1, step):.2f}")\n'})}),"\n",(0,i.jsx)(e.h2,{id:"transfer-learning-between-simulation-and-reality",children:"Transfer Learning Between Simulation and Reality"}),"\n",(0,i.jsx)(e.p,{children:"The reality gap between simulation and real-world environments poses a significant challenge for humanoid robotics. While simulation provides a safe and efficient environment for learning, behaviors learned in simulation often fail to transfer to the real world due to model inaccuracies and environmental differences."}),"\n",(0,i.jsx)(e.p,{children:"Domain randomization is a technique that aims to bridge this gap by training agents in simulations with randomized parameters, making them robust to variations between simulation and reality. This approach has shown success in enabling sim-to-real transfer for robotic manipulation and locomotion tasks."}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-cpp",children:'#include <torch/torch.h>\n#include <vector>\n#include <random>\n#include <memory>\n\nclass DomainRandomization {\nprivate:\n    std::mt19937 rng;\n    std::vector<double> mass_range;\n    std::vector<double> friction_range;\n    std::vector<double> actuator_noise_range;\n    std::vector<double> sensor_noise_range;\n\npublic:\n    DomainRandomization() : rng(std::random_device{}()) {\n        // Define ranges for randomization\n        mass_range = {0.8, 1.2};  // \xb120% mass variation\n        friction_range = {0.5, 1.5};  // Friction range\n        actuator_noise_range = {0.0, 0.05};  // 5% actuator noise\n        sensor_noise_range = {0.0, 0.02};    // 2% sensor noise\n    }\n\n    void randomize_environment(torch::Tensor& env_params) {\n        // Randomize environment parameters\n        std::uniform_real_distribution<double> mass_dist(mass_range[0], mass_range[1]);\n        std::uniform_real_distribution<double> friction_dist(friction_range[0], friction_range[1]);\n        std::uniform_real_distribution<double> noise_dist(actuator_noise_range[0], actuator_noise_range[1]);\n\n        // Apply randomization to environment parameters\n        auto options = torch::TensorOptions().dtype(torch::kFloat32);\n\n        // Mass randomization\n        auto mass_rand = torch::rand(env_params.size(0), options) * (mass_range[1] - mass_range[0]) + mass_range[0];\n        env_params.select(1, 0) *= mass_rand;  // Assuming column 0 is mass\n\n        // Friction randomization\n        auto friction_rand = torch::rand(env_params.size(0), options) * (friction_range[1] - friction_range[0]) + friction_range[0];\n        env_params.select(1, 1) *= friction_rand;  // Assuming column 1 is friction\n\n        // Add noise to other parameters\n        auto noise = torch::randn({env_params.size(0), env_params.size(1)}, options);\n        noise.select(1, 2) *= (actuator_noise_range[1] - actuator_noise_range[0]);  // Actuator noise\n        noise.select(1, 3) *= (sensor_noise_range[1] - sensor_noise_range[0]);      // Sensor noise\n        env_params += noise * 0.1;  // Small noise addition\n    }\n\n    torch::Tensor randomize_observations(torch::Tensor& obs, double noise_level = 0.02) {\n        // Add realistic noise to observations\n        auto noise = torch::randn_like(obs) * noise_level;\n        return obs + noise;\n    }\n\n    torch::Tensor randomize_actions(torch::Tensor& actions, double noise_level = 0.05) {\n        // Add realistic noise to actions (actuator noise)\n        auto noise = torch::randn_like(actions) * noise_level;\n        auto noisy_actions = actions + noise;\n\n        // Clamp actions to reasonable bounds\n        return torch::clamp(noisy_actions, -1.0, 1.0);\n    }\n};\n\nclass SkillTransferModule {\nprivate:\n    std::shared_ptr<torch::jit::script::Module> source_policy;\n    std::shared_ptr<torch::jit::script::Module> target_policy;\n    std::unique_ptr<DomainRandomization> domain_randomizer;\n    torch::optim::Adam optimizer;\n    double adaptation_lr;\n\npublic:\n    SkillTransferModule(std::shared_ptr<torch::jit::script::Module> source,\n                       int state_dim, int action_dim, double lr = 1e-4)\n        : source_policy(source), adaptation_lr(lr) {\n\n        // Initialize target policy with same architecture as source\n        target_policy = std::make_shared<torch::jit::script::Module>(*source);\n\n        // Initialize domain randomizer\n        domain_randomizer = std::make_unique<DomainRandomization>();\n\n        // Setup optimizer for target policy\n        std::vector<torch::Tensor> target_params;\n        for (const auto& param : target_policy->parameters()) {\n            target_params.push_back(param.value());\n        }\n        optimizer = torch::optim::Adam(target_params, lr);\n    }\n\n    void adapt_to_new_domain(const std::vector<torch::Tensor>& obs_batch,\n                           const std::vector<torch::Tensor>& action_batch,\n                           int adaptation_steps = 100) {\n        /*\n        Adapt the policy to a new domain using limited real-world data\n        */\n        torch::Tensor total_loss = torch::zeros({1}, torch::kFloat32);\n\n        for (int step = 0; step < adaptation_steps; ++step) {\n            optimizer.zero_grad();\n\n            // Randomly select a batch\n            int idx = std::rand() % obs_batch.size();\n            auto obs = obs_batch[idx];\n            auto expert_action = action_batch[idx];\n\n            // Get action from current target policy\n            std::vector<torch::jit::IValue> inputs;\n            inputs.push_back(obs);\n            auto policy_action_tensor = target_policy->forward(inputs).toTensor();\n\n            // Calculate behavioral cloning loss\n            auto loss = torch::mse_loss(policy_action_tensor, expert_action);\n\n            // Backpropagate\n            loss.backward();\n            optimizer.step();\n\n            total_loss += loss.item<float>();\n        }\n\n        std::cout << "Adaptation completed. Average loss: " << total_loss.item<float>() / adaptation_steps << std::endl;\n    }\n\n    torch::Tensor execute_policy(const torch::Tensor& state) {\n        /*\n        Execute the adapted policy\n        */\n        std::vector<torch::jit::IValue> inputs;\n        inputs.push_back(state);\n\n        auto result = target_policy->forward(inputs).toTensor();\n        return result;\n    }\n\n    void domain_randomization_training(torch::Tensor& env_params) {\n        /*\n        Apply domain randomization during training\n        */\n        domain_randomizer->randomize_environment(env_params);\n    }\n};\n'})}),"\n",(0,i.jsx)(e.h2,{id:"online-learning-and-adaptation",children:"Online Learning and Adaptation"}),"\n",(0,i.jsx)(e.p,{children:"Online learning enables humanoid robots to continuously adapt their behaviors based on real-time experience. This is particularly important for humanoid robots that must operate in dynamic environments where conditions can change unexpectedly."}),"\n",(0,i.jsx)(e.p,{children:"Online adaptation can take various forms, from simple parameter tuning to complex skill modification. The key challenge is to enable adaptation without disrupting ongoing tasks or compromising safety."}),"\n",(0,i.jsx)(e.h2,{id:"skill-acquisition-and-refinement",children:"Skill Acquisition and Refinement"}),"\n",(0,i.jsx)(e.p,{children:"Skill acquisition in humanoid robots involves learning complex motor behaviors that can be reused and combined to accomplish various tasks. These skills might include walking, grasping, reaching, or more complex behaviors like opening doors or pouring liquids."}),"\n",(0,i.jsx)(e.p,{children:"Hierarchical skill learning structures skills at multiple levels of abstraction, allowing for efficient learning and reuse. Primitive skills can be combined to form more complex behaviors, and abstract skills can be refined into specific implementations."}),"\n",(0,i.jsx)(e.p,{children:"[Image: Reference to diagram or illustration]"}),"\n",(0,i.jsx)(e.h2,{id:"advanced-learning-techniques",children:"Advanced Learning Techniques"}),"\n",(0,i.jsx)(e.p,{children:"Modern humanoid robots employ several advanced learning techniques:"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Meta-Learning"}),": Learning to learn quickly from few examples"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Multi-Task Learning"}),": Learning multiple related tasks simultaneously"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Curriculum Learning"}),": Gradually increasing task difficulty"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Multi-Agent Learning"}),": Learning in environments with multiple agents"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Safe Exploration"}),": Exploring new behaviors while maintaining safety"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(e.p,{children:"Learning and adaptation are essential capabilities that enable humanoid robots to improve their performance and adapt to new situations over time. The field continues to evolve with new algorithms and techniques that address the unique challenges of learning in physical systems, including safety, sample efficiency, and the reality gap between simulation and the real world. Success in humanoid learning will require continued advances in algorithms, computational efficiency, and safety that work together to create truly adaptive robotic systems."})]})}function m(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>r,x:()=>s});var a=t(6540);const i={},o=a.createContext(i);function r(n){const e=a.useContext(o);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:r(n.components),a.createElement(o.Provider,{value:e},n.children)}}}]);