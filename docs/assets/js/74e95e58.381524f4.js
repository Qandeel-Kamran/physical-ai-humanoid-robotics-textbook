"use strict";(self.webpackChunkmy_textbook_website=self.webpackChunkmy_textbook_website||[]).push([[705],{4713:(n,e,o)=>{o.r(e),o.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>m,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"chapter-12-simulation","title":"Simulation and Modeling for Physical AI","description":"Understanding physics simulation, modeling, and validation techniques for humanoid robots, including sim-to-real transfer and sensor simulation.","source":"@site/docs/chapter-12-simulation.md","sourceDirName":".","slug":"/chapter-12-simulation","permalink":"/physical-ai-humanoid-robotics-textbook/docs/chapter-12-simulation","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":12,"frontMatter":{"title":"Simulation and Modeling for Physical AI","description":"Understanding physics simulation, modeling, and validation techniques for humanoid robots, including sim-to-real transfer and sensor simulation.","sidebar_position":12,"wordCount":"1400-1700","prerequisites":"Physics simulation and modeling concepts","learningOutcomes":["Develop accurate simulation environments for humanoid robots","Implement sim-to-real transfer techniques","Validate simulation models against physical systems"],"subtopics":["Physics simulation environments","Real-to-sim and sim-to-real transfer","Contact modeling and friction simulation","Sensor simulation and noise modeling","Validation and verification of models"],"status":"draft","authors":["Textbook Author"],"reviewers":["Domain Expert"]},"sidebar":"textbookSidebar","previous":{"title":"Human-Robot Interaction and Social Robotics","permalink":"/physical-ai-humanoid-robotics-textbook/docs/chapter-11-hri"},"next":{"title":"Hardware Design and Integration","permalink":"/physical-ai-humanoid-robotics-textbook/docs/chapter-13-hardware"}}');var t=o(4848),a=o(8453);const r={title:"Simulation and Modeling for Physical AI",description:"Understanding physics simulation, modeling, and validation techniques for humanoid robots, including sim-to-real transfer and sensor simulation.",sidebar_position:12,wordCount:"1400-1700",prerequisites:"Physics simulation and modeling concepts",learningOutcomes:["Develop accurate simulation environments for humanoid robots","Implement sim-to-real transfer techniques","Validate simulation models against physical systems"],subtopics:["Physics simulation environments","Real-to-sim and sim-to-real transfer","Contact modeling and friction simulation","Sensor simulation and noise modeling","Validation and verification of models"],status:"draft",authors:["Textbook Author"],reviewers:["Domain Expert"]},s="Simulation and Modeling for Physical AI",l={},c=[{value:"Physics Simulation Environments",id:"physics-simulation-environments",level:2},{value:"Real-to-Sim and Sim-to-Real Transfer",id:"real-to-sim-and-sim-to-real-transfer",level:2},{value:"Contact Modeling and Friction Simulation",id:"contact-modeling-and-friction-simulation",level:2},{value:"Sensor Simulation and Noise Modeling",id:"sensor-simulation-and-noise-modeling",level:2},{value:"Validation and Verification of Models",id:"validation-and-verification-of-models",level:2},{value:"Advanced Simulation Techniques",id:"advanced-simulation-techniques",level:2},{value:"Summary",id:"summary",level:2}];function d(n){const e={admonition:"admonition",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,a.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"simulation-and-modeling-for-physical-ai",children:"Simulation and Modeling for Physical AI"})}),"\n",(0,t.jsx)(e.p,{children:"Simulation environments play a crucial role in the development and testing of humanoid robots, providing safe and efficient platforms for algorithm development, control system testing, and behavior learning. Unlike traditional robotics applications, humanoid robots require sophisticated simulation environments that can accurately model the complex interactions between the robot, environment, and humans."}),"\n",(0,t.jsx)(e.p,{children:"The fidelity of simulation environments directly impacts the success of sim-to-real transfer, where behaviors learned in simulation are applied to real robots. Achieving high-fidelity simulation requires careful modeling of physics, contact mechanics, sensor characteristics, and environmental factors that influence robot behavior."}),"\n",(0,t.jsx)(e.h2,{id:"physics-simulation-environments",children:"Physics Simulation Environments"}),"\n",(0,t.jsx)(e.p,{children:"Physics simulation engines form the foundation of humanoid robot simulation, providing the computational framework for modeling rigid body dynamics, contact mechanics, and environmental interactions. Popular simulation platforms for humanoid robotics include Gazebo, PyBullet, MuJoCo, Isaac Gym, and Webots, each with specific strengths for different aspects of humanoid robot development."}),"\n",(0,t.jsx)(e.p,{children:"High-fidelity physics simulation must accurately model complex phenomena such as friction, contact dynamics, and material properties. This is particularly challenging for humanoid robots due to their complex morphology, multiple contact points, and the need to maintain balance during dynamic interactions."}),"\n",(0,t.jsx)(e.p,{children:"Modern simulation environments often include GPU-accelerated physics computation to enable large-scale parallel simulation, which is particularly valuable for reinforcement learning applications where many simulation instances run simultaneously to accelerate learning."}),"\n",(0,t.jsx)(e.admonition,{type:"tip",children:(0,t.jsx)(e.p,{children:"When selecting a physics simulation engine for humanoid robotics, consider the trade-off between accuracy and computational efficiency. For control system development, accuracy may be more important, while for learning applications, efficiency might be prioritized."})}),"\n",(0,t.jsx)(e.h2,{id:"real-to-sim-and-sim-to-real-transfer",children:"Real-to-Sim and Sim-to-Real Transfer"}),"\n",(0,t.jsx)(e.p,{children:"The reality gap between simulation and real-world environments remains one of the most significant challenges in humanoid robotics. Sim-to-real transfer techniques aim to bridge this gap by ensuring that behaviors learned in simulation can be successfully applied to real robots."}),"\n",(0,t.jsx)(e.p,{children:"Domain randomization is a prominent technique that addresses the reality gap by training robots in simulations with randomized parameters, making them robust to variations between simulation and reality. This approach has proven effective for enabling sim-to-real transfer of control policies for locomotion and manipulation tasks."}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"import numpy as np\nimport pybullet as p\nimport pybullet_data\nfrom scipy.spatial.transform import Rotation as R\nimport matplotlib.pyplot as plt\n\nclass PhysicsSimulator:\n    \"\"\"\n    Physics simulation environment for humanoid robot\n    \"\"\"\n    def __init__(self, use_gui=True, gravity=[0, 0, -9.81]):\n        if use_gui:\n            self.physics_client = p.connect(p.GUI)\n        else:\n            self.physics_client = p.connect(p.DIRECT)\n\n        p.setAdditionalSearchPath(pybullet_data.getDataPath())\n        p.setGravity(*gravity)\n\n        # Store simulation parameters\n        self.sim_params = {\n            'time_step': 1/240.0,\n            'num_solver_iterations': 10,\n            'friction_anchor': 0.01\n        }\n\n        p.setTimeStep(self.sim_params['time_step'])\n        p.setPhysicsEngineParameter(numSolverIterations=self.sim_params['num_solver_iterations'])\n\n        # Load ground plane\n        self.ground_id = p.loadURDF(\"plane.urdf\")\n\n        # Store robot information\n        self.robot_id = None\n        self.joint_info = {}\n        self.link_info = {}\n\n    def load_humanoid_robot(self, urdf_path, start_position=[0, 0, 1], start_orientation=[0, 0, 0, 1]):\n        \"\"\"\n        Load humanoid robot URDF into simulation\n        \"\"\"\n        self.robot_id = p.loadURDF(\n            urdf_path,\n            start_position,\n            start_orientation,\n            flags=p.URDF_USE_SELF_COLLISION\n        )\n\n        # Get joint information\n        num_joints = p.getNumJoints(self.robot_id)\n        for i in range(num_joints):\n            joint_info = p.getJointInfo(self.robot_id, i)\n            self.joint_info[joint_info[1].decode('utf-8')] = {\n                'index': i,\n                'type': joint_info[2],\n                'lower_limit': joint_info[8],\n                'upper_limit': joint_info[9],\n                'max_force': joint_info[10],\n                'max_velocity': joint_info[11]\n            }\n\n        return self.robot_id\n\n    def get_robot_state(self):\n        \"\"\"\n        Get current state of the robot\n        \"\"\"\n        if self.robot_id is None:\n            return None\n\n        state = {\n            'joint_positions': [],\n            'joint_velocities': [],\n            'joint_torques': [],\n            'base_position': [],\n            'base_orientation': [],\n            'base_linear_velocity': [],\n            'base_angular_velocity': []\n        }\n\n        # Get joint states\n        for joint_name, info in self.joint_info.items():\n            joint_state = p.getJointState(self.robot_id, info['index'])\n            state['joint_positions'].append(joint_state[0])\n            state['joint_velocities'].append(joint_state[1])\n            state['joint_torques'].append(joint_state[3])\n\n        # Get base (root link) state\n        base_pos, base_orn = p.getBasePositionAndOrientation(self.robot_id)\n        base_lin_vel, base_ang_vel = p.getBaseVelocity(self.robot_id)\n\n        state['base_position'] = list(base_pos)\n        state['base_orientation'] = list(base_orn)\n        state['base_linear_velocity'] = list(base_lin_vel)\n        state['base_angular_velocity'] = list(base_ang_vel)\n\n        return state\n\n    def apply_joint_torques(self, torques):\n        \"\"\"\n        Apply torques to robot joints\n        \"\"\"\n        if self.robot_id is None:\n            return\n\n        # Ensure torques list matches number of controllable joints\n        torque_list = list(torques)\n        num_joints = len([j for j in self.joint_info.values() if j['type'] in [p.JOINT_REVOLUTE, p.JOINT_PRISMATIC]])\n\n        if len(torque_list) != num_joints:\n            raise ValueError(f\"Expected {num_joints} torques, got {len(torque_list)}\")\n\n        # Apply torques\n        p.setJointMotorControlArray(\n            self.robot_id,\n            [info['index'] for info in self.joint_info.values()],\n            p.TORQUE_CONTROL,\n            forces=torque_list\n        )\n\n    def step_simulation(self):\n        \"\"\"\n        Step the simulation forward\n        \"\"\"\n        p.stepSimulation()\n\n    def reset_robot(self, position=[0, 0, 1], orientation=[0, 0, 0, 1]):\n        \"\"\"\n        Reset robot to initial position\n        \"\"\"\n        if self.robot_id is not None:\n            p.resetBasePositionAndOrientation(self.robot_id, position, orientation)\n            # Reset joint positions to zero\n            for joint_name, info in self.joint_info.items():\n                if info['type'] in [p.JOINT_REVOLUTE, p.JOINT_PRISMATIC]:\n                    p.resetJointState(self.robot_id, info['index'], 0, 0)\n\n    def disconnect(self):\n        \"\"\"\n        Disconnect from physics engine\n        \"\"\"\n        p.disconnect(self.physics_client)\n\nclass DomainRandomization:\n    \"\"\"\n    Domain randomization for sim-to-real transfer\n    \"\"\"\n    def __init__(self, randomization_ranges=None):\n        if randomization_ranges is None:\n            # Default randomization ranges\n            self.ranges = {\n                'robot_mass': [0.8, 1.2],  # \xb120% mass variation\n                'link_damping': [0.0, 0.1],  # Joint damping\n                'friction': [0.5, 1.5],  # Friction coefficient\n                'restitution': [0.0, 0.2],  # Restitution coefficient\n                'gravity': [0.9, 1.1],  # Gravity scaling\n                'actuator_noise': [0.0, 0.05],  # Actuator noise\n                'sensor_noise': [0.0, 0.02],  # Sensor noise\n                'floor_friction': [0.5, 1.5],  # Floor friction\n                'object_properties': [0.8, 1.2]  # Object properties scaling\n            }\n        else:\n            self.ranges = randomization_ranges\n\n        self.current_params = self._sample_randomization()\n\n    def _sample_randomization(self):\n        \"\"\"\n        Sample randomization parameters\n        \"\"\"\n        params = {}\n        for param_name, range_vals in self.ranges.items():\n            if isinstance(range_vals, (list, tuple)) and len(range_vals) == 2:\n                # Sample uniformly from range\n                min_val, max_val = range_vals\n                params[param_name] = np.random.uniform(min_val, max_val)\n            else:\n                # Use the value directly if it's not a range\n                params[param_name] = range_vals\n\n        return params\n\n    def apply_randomization(self, simulator, robot_id):\n        \"\"\"\n        Apply domain randomization to the simulation\n        \"\"\"\n        # Update randomization parameters\n        self.current_params = self._sample_randomization()\n\n        # Apply mass randomization\n        mass_scale = self.current_params['robot_mass']\n        self._apply_mass_randomization(robot_id, mass_scale)\n\n        # Apply friction randomization\n        friction = self.current_params['friction']\n        self._apply_friction_randomization(simulator, friction)\n\n        # Apply gravity randomization\n        gravity_scale = self.current_params['gravity']\n        p.setGravity(0, 0, -9.81 * gravity_scale)\n\n        # Apply link damping\n        damping = self.current_params['link_damping']\n        self._apply_damping_randomization(robot_id, damping)\n\n        return self.current_params\n\n    def _apply_mass_randomization(self, robot_id, scale_factor):\n        \"\"\"\n        Apply mass scaling to robot links\n        \"\"\"\n        # This is a simplified implementation\n        # In practice, you'd need to reload the URDF with modified masses\n        # or use PyBullet's mass modification functions if available\n        pass\n\n    def _apply_friction_randomization(self, simulator, friction_coeff):\n        \"\"\"\n        Apply friction randomization\n        \"\"\"\n        # Modify floor friction\n        p.changeDynamics(\n            simulator.ground_id,\n            -1,\n            lateralFriction=friction_coeff,\n            rollingFriction=0.005,\n            spinningFriction=0.005\n        )\n\n        # Modify robot-ground friction\n        for link_idx in range(p.getNumJoints(robot_id)):\n            p.changeDynamics(\n                robot_id,\n                link_idx,\n                lateralFriction=friction_coeff,\n                rollingFriction=0.005,\n                spinningFriction=0.005\n            )\n\n    def _apply_damping_randomization(self, robot_id, damping_coeff):\n        \"\"\"\n        Apply joint damping randomization\n        \"\"\"\n        for joint_name, info in simulator.joint_info.items():\n            if info['type'] in [p.JOINT_REVOLUTE, p.JOINT_PRISMATIC]:\n                # PyBullet doesn't directly support setting joint damping\n                # This would require modifying the URDF or using a custom controller\n                pass\n\n    def add_sensor_noise(self, sensor_data):\n        \"\"\"\n        Add noise to sensor data to simulate real sensors\n        \"\"\"\n        noise_level = self.current_params['sensor_noise']\n        noise = np.random.normal(0, noise_level, size=sensor_data.shape)\n        return sensor_data + noise\n\n    def add_actuator_noise(self, commanded_torques):\n        \"\"\"\n        Add noise to actuator commands\n        \"\"\"\n        noise_level = self.current_params['actuator_noise']\n        noise = np.random.normal(0, noise_level, size=commanded_torques.shape)\n        noisy_torques = commanded_torques + noise\n        return np.clip(noisy_torques, -max_torque, max_torque)  # Clip to safe limits\n\nclass SensorSimulator:\n    \"\"\"\n    Simulate various sensors for humanoid robots\n    \"\"\"\n    def __init__(self, robot_id, simulator):\n        self.robot_id = robot_id\n        self.simulator = simulator\n        self.domain_randomizer = DomainRandomization()\n\n        # Sensor parameters\n        self.camera_params = {\n            'image_width': 640,\n            'image_height': 480,\n            'fov': 60,\n            'near_plane': 0.1,\n            'far_plane': 10.0\n        }\n\n    def get_joint_sensors(self):\n        \"\"\"\n        Get joint position, velocity, and effort sensors\n        \"\"\"\n        joint_states = p.getJointStates(\n            self.robot_id,\n            [info['index'] for info in self.simulator.joint_info.values()]\n        )\n\n        positions = [state[0] for state in joint_states]\n        velocities = [state[1] for state in joint_states]\n        efforts = [state[3] for state in joint_states]\n\n        # Add noise to simulate real sensors\n        positions = self.domain_randomizer.add_sensor_noise(np.array(positions))\n        velocities = self.domain_randomizer.add_sensor_noise(np.array(velocities))\n        efforts = self.domain_randomizer.add_sensor_noise(np.array(efforts))\n\n        return {\n            'positions': positions,\n            'velocities': velocities,\n            'efforts': efforts\n        }\n\n    def get_imu_data(self):\n        \"\"\"\n        Simulate IMU data (orientation, angular velocity, linear acceleration)\n        \"\"\"\n        # Get base orientation and velocity\n        pos, orn = p.getBasePositionAndOrientation(self.robot_id)\n        lin_vel, ang_vel = p.getBaseVelocity(self.robot_id)\n\n        # Convert to IMU-like readings\n        # Orientation as quaternion\n        orientation = np.array(orn)\n\n        # Angular velocity (already in the right format)\n        angular_velocity = np.array(ang_vel)\n\n        # Linear acceleration (derivative of velocity, simplified)\n        # In real implementation, this would require tracking previous velocities\n        gravity = np.array([0, 0, -9.81])\n        # Subtract gravity to get linear acceleration\n        linear_acceleration = np.array(lin_vel) * 10  # Simplified, not physically accurate\n        linear_acceleration[2] += 9.81  # Add back gravity component\n\n        # Add sensor noise\n        orientation = self.domain_randomizer.add_sensor_noise(orientation)\n        angular_velocity = self.domain_randomizer.add_sensor_noise(angular_velocity)\n        linear_acceleration = self.domain_randomizer.add_sensor_noise(linear_acceleration)\n\n        return {\n            'orientation': orientation,\n            'angular_velocity': angular_velocity,\n            'linear_acceleration': linear_acceleration\n        }\n\n    def get_force_torque_sensors(self, link_index):\n        \"\"\"\n        Get force/torque sensor data from specific link\n        \"\"\"\n        # Get contact information\n        contact_points = p.getContactPoints(bodyA=self.robot_id, linkIndexA=link_index)\n\n        total_normal_force = 0\n        total_lateral_force = 0\n        total_torque = np.zeros(3)\n\n        for contact in contact_points:\n            # Extract contact forces\n            normal_force = contact[9]  # Normal force magnitude\n            lateral_force1 = contact[10]  # Lateral friction force 1\n            lateral_force2 = contact[11]  # Lateral friction force 2\n\n            total_normal_force += normal_force\n            total_lateral_force += np.sqrt(lateral_force1**2 + lateral_force2**2)\n\n            # Calculate torque contribution (simplified)\n            contact_pos = np.array(contact[5])  # Contact position\n            link_pos, _ = p.getLinkState(self.robot_id, link_index)[:2]\n            lever_arm = contact_pos - np.array(link_pos)\n            torque_contribution = np.cross(lever_arm, np.array([lateral_force1, lateral_force2, normal_force]))\n            total_torque += torque_contribution\n\n        # Add sensor noise\n        total_normal_force += np.random.normal(0, 0.1)\n        total_lateral_force += np.random.normal(0, 0.05)\n        total_torque += np.random.normal(0, 0.01, size=3)\n\n        return {\n            'normal_force': total_normal_force,\n            'lateral_force': total_lateral_force,\n            'torque': total_torque\n        }\n\n    def get_camera_image(self, camera_pos, target_pos):\n        \"\"\"\n        Get RGB and depth image from camera\n        \"\"\"\n        # Calculate view matrix\n        view_matrix = p.computeViewMatrix(camera_pos, target_pos, [0, 0, 1])\n\n        # Calculate projection matrix\n        aspect_ratio = self.camera_params['image_width'] / self.camera_params['image_height']\n        projection_matrix = p.computeProjectionMatrixFOV(\n            self.camera_params['fov'],\n            aspect_ratio,\n            self.camera_params['near_plane'],\n            self.camera_params['far_plane']\n        )\n\n        # Render image\n        _, _, rgba, depth, seg_mask = p.getCameraImage(\n            width=self.camera_params['image_width'],\n            height=self.camera_params['image_height'],\n            viewMatrix=view_matrix,\n            projectionMatrix=projection_matrix,\n            renderer=p.ER_BULLET_HARDWARE_OPENGL\n        )\n\n        # Extract RGB and convert depth to meters\n        rgb_image = np.reshape(rgba, (self.camera_params['image_height'], self.camera_params['image_width'], 4))[:, :, :3]\n        depth_image = self._convert_depth_to_meters(depth, self.camera_params['near_plane'], self.camera_params['far_plane'])\n\n        return {\n            'rgb': rgb_image,\n            'depth': depth_image,\n            'segmentation': seg_mask\n        }\n\n    def _convert_depth_to_meters(self, depth_buffer, near_plane, far_plane):\n        \"\"\"\n        Convert PyBullet's depth buffer to meters\n        \"\"\"\n        depth_image = np.array(depth_buffer)\n        z_buffer = depth_image\n        depth_meters = (2.0 * near_plane * far_plane) / (\n            near_plane + far_plane - (z_buffer - 0.5) * 2.0 * (far_plane - near_plane)\n        )\n        return depth_meters\n\nclass ModelValidator:\n    \"\"\"\n    Validate simulation models against real-world behavior\n    \"\"\"\n    def __init__(self):\n        self.validation_metrics = {\n            'position_error': [],\n            'velocity_error': [],\n            'force_error': [],\n            'timing_accuracy': []\n        }\n\n    def compare_trajectories(self, sim_trajectory, real_trajectory):\n        \"\"\"\n        Compare simulated and real robot trajectories\n        \"\"\"\n        if len(sim_trajectory) != len(real_trajectory):\n            raise ValueError(\"Trajectories must have the same length\")\n\n        position_errors = []\n        velocity_errors = []\n\n        for i in range(len(sim_trajectory)):\n            sim_state = sim_trajectory[i]\n            real_state = real_trajectory[i]\n\n            # Calculate position error\n            pos_error = np.linalg.norm(\n                np.array(sim_state['position']) - np.array(real_state['position'])\n            )\n            position_errors.append(pos_error)\n\n            # Calculate velocity error\n            vel_error = np.linalg.norm(\n                np.array(sim_state['velocity']) - np.array(real_state['velocity'])\n            )\n            velocity_errors.append(vel_error)\n\n        avg_pos_error = np.mean(position_errors)\n        avg_vel_error = np.mean(velocity_errors)\n\n        self.validation_metrics['position_error'].append(avg_pos_error)\n        self.validation_metrics['velocity_error'].append(avg_vel_error)\n\n        return {\n            'avg_position_error': avg_pos_error,\n            'avg_velocity_error': avg_vel_error,\n            'max_position_error': np.max(position_errors),\n            'max_velocity_error': np.max(velocity_errors)\n        }\n\n    def validate_contact_dynamics(self, sim_forces, real_forces):\n        \"\"\"\n        Validate contact force predictions\n        \"\"\"\n        force_errors = []\n        for sim_f, real_f in zip(sim_forces, real_forces):\n            error = np.abs(sim_f - real_f)\n            force_errors.append(error)\n\n        avg_force_error = np.mean(force_errors)\n        self.validation_metrics['force_error'].append(avg_force_error)\n\n        return {\n            'avg_force_error': avg_force_error,\n            'std_force_error': np.std(force_errors)\n        }\n\n    def plot_validation_results(self):\n        \"\"\"\n        Plot validation results\n        \"\"\"\n        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n        # Position error\n        axes[0, 0].plot(self.validation_metrics['position_error'])\n        axes[0, 0].set_title('Position Error Over Time')\n        axes[0, 0].set_xlabel('Validation Instance')\n        axes[0, 0].set_ylabel('Error (m)')\n\n        # Velocity error\n        axes[0, 1].plot(self.validation_metrics['velocity_error'])\n        axes[0, 1].set_title('Velocity Error Over Time')\n        axes[0, 1].set_xlabel('Validation Instance')\n        axes[0, 1].set_ylabel('Error (m/s)')\n\n        # Force error\n        axes[1, 0].plot(self.validation_metrics['force_error'])\n        axes[1, 0].set_title('Force Error Over Time')\n        axes[1, 0].set_xlabel('Validation Instance')\n        axes[1, 0].set_ylabel('Error (N)')\n\n        # Timing accuracy\n        if self.validation_metrics['timing_accuracy']:\n            axes[1, 1].plot(self.validation_metrics['timing_accuracy'])\n            axes[1, 1].set_title('Timing Accuracy Over Time')\n            axes[1, 1].set_xlabel('Validation Instance')\n            axes[1, 1].set_ylabel('Accuracy')\n\n        plt.tight_layout()\n        plt.show()\n"})}),"\n",(0,t.jsx)(e.h2,{id:"contact-modeling-and-friction-simulation",children:"Contact Modeling and Friction Simulation"}),"\n",(0,t.jsx)(e.p,{children:"Accurate contact modeling is crucial for humanoid robots, as their operation involves frequent and complex interactions with the environment through feet, hands, and other body parts. Contact models must accurately represent the physics of contact, including normal forces, friction, and impact dynamics."}),"\n",(0,t.jsx)(e.p,{children:"The challenge in contact modeling for humanoid robots stems from the need to handle multiple simultaneous contacts, varying contact surfaces, and the transition between contact and non-contact states. Different approaches to contact modeling include penalty methods, constraint-based methods, and impulse-based methods."}),"\n",(0,t.jsx)(e.p,{children:"Friction modeling is particularly important for humanoid robots, as it affects their ability to maintain balance, manipulate objects, and walk stably. Accurate friction models must account for static and dynamic friction, as well as the Stribeck effect in some applications."}),"\n",(0,t.jsx)(e.h2,{id:"sensor-simulation-and-noise-modeling",children:"Sensor Simulation and Noise Modeling"}),"\n",(0,t.jsx)(e.p,{children:"Sensor simulation is essential for developing and testing perception and control systems in humanoid robots. Simulated sensors must reproduce the characteristics of real sensors, including noise, latency, and physical limitations."}),"\n",(0,t.jsx)(e.p,{children:"Common sensors for humanoid robots include cameras for vision, IMUs for orientation and acceleration, force/torque sensors for contact detection, and joint encoders for position feedback. Each sensor type requires specific modeling approaches to accurately simulate its real-world behavior."}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-cpp",children:"#include <dart/dart.hpp>\n#include <dart/gui/osg/osg.hpp>\n#include <Eigen/Dense>\n#include <random>\n#include <memory>\n\nclass SensorNoiseModel {\nprivate:\n    std::random_device rd;\n    std::mt19937 gen;\n    std::normal_distribution<double> noise_dist;\n\npublic:\n    SensorNoiseModel(double mean = 0.0, double stddev = 0.01)\n        : gen(rd()), noise_dist(mean, stddev) {}\n\n    double add_noise(double measurement) {\n        return measurement + noise_dist(gen);\n    }\n\n    Eigen::VectorXd add_noise_vec(const Eigen::VectorXd& measurement) {\n        Eigen::VectorXd noisy = measurement;\n        for (int i = 0; i < noisy.size(); ++i) {\n            noisy[i] += noise_dist(gen);\n        }\n        return noisy;\n    }\n\n    void set_noise_parameters(double mean, double stddev) {\n        noise_dist = std::normal_distribution<double>(mean, stddev);\n    }\n};\n\nclass SimulatedIMU {\nprivate:\n    Eigen::Vector3d true_angular_velocity;\n    Eigen::Vector3d true_linear_acceleration;\n    Eigen::Quaterniond true_orientation;\n\n    SensorNoiseModel gyro_noise;\n    SensorNoiseModel accel_noise;\n    SensorNoiseModel mag_noise;\n\n    double bias_drift_gyro;\n    double bias_drift_accel;\n\npublic:\n    SimulatedIMU() : gyro_noise(0.0, 0.01), accel_noise(0.0, 0.02), mag_noise(0.0, 0.005),\n                     bias_drift_gyro(0.0), bias_drift_accel(0.0) {}\n\n    struct IMUReading {\n        Eigen::Vector3d angular_velocity;\n        Eigen::Vector3d linear_acceleration;\n        Eigen::Quaterniond orientation;\n        double temperature;\n    };\n\n    IMUReading get_reading(const dart::dynamics::SkeletonPtr& robot, size_t link_idx) {\n        // Get the link's transform and velocity\n        auto link = robot->getBodyNode(link_idx);\n        auto transform = link->getTransform();\n        auto spatial_velocity = link->getSpatialVelocity();\n\n        // Extract linear and angular components\n        Eigen::Vector3d linear_velocity = spatial_velocity.linear();\n        Eigen::Vector3d angular_velocity = spatial_velocity.angular();\n\n        // Calculate linear acceleration (simplified - would need proper derivative in practice)\n        Eigen::Vector3d gravity(0, 0, -9.81);\n        Eigen::Vector3d linear_acceleration = gravity;  // Just gravity in base frame initially\n\n        // Add sensor noise\n        IMUReading reading;\n        reading.angular_velocity = gyro_noise.add_noise_vec(angular_velocity);\n        reading.linear_acceleration = accel_noise.add_noise_vec(linear_acceleration);\n        reading.orientation = Eigen::Quaterniond(transform.rotation());  // True orientation\n        reading.temperature = 25.0 + 0.1 * noise_dist(gen);  // Temperature with small variation\n\n        return reading;\n    }\n\n    void update_bias_drift() {\n        // Simulate slow bias drift over time\n        bias_drift_gyro += 0.001 * (noise_dist(gen) - 0.5);  // Slow drift\n        bias_drift_accel += 0.0005 * (noise_dist(gen) - 0.5);\n    }\n};\n\nclass ContactSensor {\nprivate:\n    std::vector<dart::collision::Contact> last_contacts;\n    SensorNoiseModel force_noise;\n\npublic:\n    ContactSensor() : force_noise(0.0, 0.1) {}  // 0.1N noise in force measurements\n\n    struct ContactInfo {\n        Eigen::Vector3d contact_point;\n        Eigen::Vector3d normal_force;\n        Eigen::Vector3d tangential_force;\n        double pressure;\n        bool contact_present;\n    };\n\n    std::vector<ContactInfo> get_contact_information(const dart::dynamics::SkeletonPtr& robot) {\n        std::vector<ContactInfo> contacts;\n\n        // Access the collision detector to get contact information\n        auto collision_detector = robot->getCollisionDetector();\n        auto collision_group = collision_detector->createCollisionGroupShared();\n\n        // This is a simplified approach - in practice, you'd use DART's collision detection\n        // For now, we'll simulate contact detection based on proximity\n\n        for (size_t i = 0; i < robot->getNumBodyNodes(); ++i) {\n            auto body_node = robot->getBodyNode(i);\n\n            // Check if this body node is close to the ground or other objects\n            Eigen::Vector3d pos = body_node->getTransform().translation();\n\n            // Simplified ground contact detection\n            if (pos[2] < 0.01) {  // Very close to ground\n                ContactInfo ci;\n                ci.contact_point = pos;\n                ci.normal_force = Eigen::Vector3d(0, 0, 100);  // Simplified normal force\n                ci.tangential_force = Eigen::Vector3d(10, 5, 0);  // Simplified tangential force\n                ci.pressure = 100000;  // Simplified pressure (Pa)\n                ci.contact_present = true;\n\n                // Add noise to force measurements\n                ci.normal_force = force_noise.add_noise_vec(ci.normal_force);\n                ci.tangential_force = force_noise.add_noise_vec(ci.tangential_force);\n\n                contacts.push_back(ci);\n            }\n        }\n\n        return contacts;\n    }\n};\n\nclass SimulationManager {\nprivate:\n    dart::simulation::WorldPtr world;\n    std::shared_ptr<SimulatedIMU> imu_sim;\n    std::shared_ptr<ContactSensor> contact_sim;\n    SensorNoiseModel process_noise;\n\npublic:\n    SimulationManager() : process_noise(0.0, 0.001) {\n        world = dart::simulation::World::create();\n        imu_sim = std::make_shared<SimulatedIMU>();\n        contact_sim = std::make_shared<ContactSensor>();\n\n        // Set gravity\n        world->setGravity(Eigen::Vector3d(0.0, -9.81, 0.0));\n    }\n\n    void setup_humanoid_robot(const std::string& urdf_path) {\n        // Load robot from URDF (simplified - would need proper URDF parser)\n        // In practice, you'd use DART's URDF loader\n        auto robot = dart::utils::DartLoader().parseSkeleton(urdf_path, world.get());\n\n        if (robot) {\n            world->addSkeleton(robot);\n            std::cout << \"Loaded robot with \" << robot->getNumDofs() << \" degrees of freedom\" << std::endl;\n        } else {\n            std::cerr << \"Failed to load robot from URDF\" << std::endl;\n        }\n    }\n\n    void simulate_step(double dt) {\n        // Set time step\n        world->setTimeStep(dt);\n\n        // Integrate the world forward by one step\n        world->step();\n\n        // Update sensor simulations\n        imu_sim->update_bias_drift();\n    }\n\n    void apply_control_inputs(const std::vector<double>& torques) {\n        auto robot = world->getSkeleton(0);  // Assuming first skeleton is the robot\n        if (robot && torques.size() == robot->getNumDofs()) {\n            robot->setForces(Eigen::VectorXd::Map(torques.data(), torques.size()));\n        }\n    }\n\n    std::vector<double> get_sensor_readings(size_t robot_index = 0) {\n        auto robot = world->getSkeleton(robot_index);\n        if (!robot) return {};\n\n        std::vector<double> sensor_data;\n\n        // Get joint positions\n        for (size_t i = 0; i < robot->getNumDofs(); ++i) {\n            double pos = robot->getPosition(i);\n            pos += process_noise.add_noise(0.0);  // Add small noise\n            sensor_data.push_back(pos);\n        }\n\n        // Get joint velocities\n        for (size_t i = 0; i < robot->getNumDofs(); ++i) {\n            double vel = robot->getVelocity(i);\n            vel += process_noise.add_noise(0.0);\n            sensor_data.push_back(vel);\n        }\n\n        return sensor_data;\n    }\n\n    double get_simulation_time() const {\n        return world->getTime();\n    }\n\n    dart::simulation::WorldPtr get_world() {\n        return world;\n    }\n};\n"})}),"\n",(0,t.jsx)(e.h2,{id:"validation-and-verification-of-models",children:"Validation and Verification of Models"}),"\n",(0,t.jsx)(e.p,{children:"Model validation is critical for ensuring that simulation results accurately reflect real-world behavior. This involves comparing simulation outputs with real-world measurements and adjusting models to minimize discrepancies."}),"\n",(0,t.jsx)(e.p,{children:"Validation approaches include system identification techniques to determine model parameters, comparison of dynamic responses, and statistical validation of stochastic behaviors. The goal is to ensure that the simulation model captures the essential dynamics and behaviors relevant to the intended application."}),"\n",(0,t.jsx)(e.p,{children:"[Image: Reference to diagram or illustration]"}),"\n",(0,t.jsx)(e.h2,{id:"advanced-simulation-techniques",children:"Advanced Simulation Techniques"}),"\n",(0,t.jsx)(e.p,{children:"Modern humanoid robotics simulation employs several advanced techniques:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"GPU Acceleration"}),": Leveraging graphics processors for parallel physics simulation"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Reduced-Order Modeling"}),": Simplifying complex models while preserving essential dynamics"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Adaptive Simulation"}),": Adjusting simulation parameters based on system behavior"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Multi-Fidelity Simulation"}),": Combining different levels of model fidelity"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Digital Twins"}),": Creating real-time synchronized simulation models"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(e.p,{children:"Simulation and modeling are fundamental to humanoid robot development, enabling safe and efficient algorithm development, testing, and validation. The challenge lies in creating simulation environments that are both accurate enough to yield meaningful results and efficient enough to support the computational demands of modern robot learning and control approaches. Success in simulation-to-real transfer requires careful attention to the reality gap and the implementation of techniques like domain randomization to ensure robust performance across simulation and real-world environments."})]})}function m(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}},8453:(n,e,o)=>{o.d(e,{R:()=>r,x:()=>s});var i=o(6540);const t={},a=i.createContext(t);function r(n){const e=i.useContext(a);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:r(n.components),i.createElement(a.Provider,{value:e},n.children)}}}]);